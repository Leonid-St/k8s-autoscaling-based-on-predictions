* [Create jobs in {{ dataproc-name }}](../../data-proc/operations/jobs.md). Once created, they will run automatically.
* [Run Apache Hive jobs](../../data-proc/tutorials/how-to-use-hive.md) using the {{ yandex-cloud }} CLI or Hive CLI.
* [Run Spark or PySpark applications](../../data-proc/tutorials/run-spark-job.md) using Spark Shell, `spark-submit`, or the {{ yandex-cloud }} CLI.
* Use `spark-submit` to [run jobs from remote hosts](../../data-proc/tutorials/remote-run-job.md) that are not part of the {{ dataproc-name }} cluster.
* Set up integration with [{{ maf-full-name }}](../../data-proc/tutorials/airflow-automation.md) or [{{ ml-platform-full-name }}](../../data-proc/tutorials/datasphere-integration.md). This will automate running the jobs.

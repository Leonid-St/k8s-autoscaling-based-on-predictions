# Повторный запуск заданий {{ ds-jobs }}

В {{ ml-platform-full-name }} доступен повторный запуск задания с переопределением необходимых параметров. Повторный запуск создает клон задания (job fork), а оригинальное задание становится родительским. Клон задания также можно запустить повторно, тогда задание станет одновременно и клоном одного, и родительским другого.

Чтобы реализовать [регулярный запуск](airflow.md) одного и того же задания с переопределением некоторых параметров, {{ ds-jobs }} можно использовать совместно с [{{ maf-full-name }}](../../../managed-airflow/).

Для повторного запуска задания с новыми параметрами в [{{ ds-cli }}](cli.md) и в составе {{ ds-jobs }} SDK доступна команда `fork`, которая позволяет переопределить следующие параметры:

* `name` — имя задания;
* `desc` — описание задания;
* `args` — аргументы задания;
* `vars` — файлы с входными и выходными данными;
* `env_vars` — переменные окружения;
* `working_storage` — конфигурация расширенной рабочей директории;
* `cloud_instance_types` — [конфигурация вычислительных ресурсов](../configurations.md).

#### Пример {#example-fork}

Рассмотрим [файл конфигурации задания](index.md#config) `config.yaml` для кода, запускающего поиск подстроки (`grep`) по входному файлу:

```yaml
name: simple-bash-script
desc: Find text pattern in input file with grep
cmd: grep -C ${RANGE} ${OPTIONS} -f ${PATTERN} ${INPUT} > ${OUTPUT}
args:
  RANGE: 0
  OPTIONS: "-h -r"
inputs:
  - pattern.txt: PATTERN
  - input.txt: INPUT
outputs:
  - output.txt: OUTPUT
```

Где:

* `RANGE` — интервал вывода поиска.
* `OPTIONS` — дополнительные флаги команды `grep`.
* `PATTERN` — файл с паттерном подстроки.
* `INPUT` — файл с входными данными.
* `OUTPUT` — файл с выходными данными.

После запуска задания его идентификатор можно получить из [логов CLI](cli.md#logs), с помощью команды `execute` или на странице проекта во вкладке {{ ds-jobs }} в браузере. Чтобы повторно запустить это задание с помощью команды SDK `fork`, укажите его идентификатор и переопределите необходимые параметры. Например, задайте новый интервал вывода поиска и новый файл с входными данными:

```python
from datasphere import SDK

sdk = SDK()

sdk.fork_job(
  '<идентификатор_задания>',
  args={'RANGE': '1'},
  vars={'INPUT': 'new_input.txt'},
)
```

## Время жизни данных задания {#ttl}

По умолчанию данные заданий удаляются через 14 дней. Если они будут удалены, повторный запуск задания станет невозможным. Вы можете изменить время жизни данных задания, выполнив команду:

```bash
datasphere project job set-data-ttl --id <идентификатор_задания> --days <количество_дней>
```

Где:

`--id` — идентификатор задания.
`--days` — количество дней, по прошествии которых данные задания будут удалены.

#### См. также {#see-also}

* [{#T}](./index.md)
* [{#T}](./airflow.md)
* [{#T}](./cli.md)